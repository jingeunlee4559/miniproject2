#!/usr/bin/env python3

import os
import json
from typing import List, Dict, Any, Optional
from langchain_chroma import Chroma

class JejuTravelRetriever:
    """
    ì˜ì¡´ì„± ì£¼ìž…: ì´ë¯¸ ìƒì„±ëœ Chroma vectorstore ì¸ìŠ¤í„´ìŠ¤ë§Œ ë°›ìŒ.
    """
    def __init__(self, vectorstore: Chroma):
        self.vs = vectorstore
        if hasattr(self.vs, "as_retriever"):
            try:
                self.retriever = self.vs.as_retriever(search_kwargs={"k": 5})
            except Exception:
                self.retriever = None
        else:
            self.retriever = None
    
    def build_query_text(self, query_json: Dict[str, Any]) -> str:
        destination = query_json.get("destination", "")
        season = query_json.get("season", "")
        companion = query_json.get("companion", "")
        travel_concept = query_json.get("TravelConcept", "")
        
        # ìžì—°ì–´ ë§¤í•‘
        season_map = {
            "ë´„": "ë´„",
            "ì—¬ë¦„": "ì—¬ë¦„", 
            "ê°€ì„": "ê°€ì„",
            "ê²¨ìš¸": "ê²¨ìš¸"
        }
        
        companion_map = {
            "í˜¼ìžì—¬í–‰": "í˜¼ìž",
            "ì»¤í”Œì—¬í–‰": "ì—°ì¸",
            "ê°€ì¡±ì—¬í–‰": "ê°€ì¡±",
            "ìš°ì •ì—¬í–‰": "ì¹œêµ¬"
        }
        
        concept_map = {
            "ì•¡í‹°ë¹„í‹°": "ì•¡í‹°ë¹„í‹° ì²´í—˜",
            "ížë§": "ížë§ íœ´ì–‘",
            "ë¬¸í™”": "ë¬¸í™” ê´€ê´‘",
            "ìžì—°": "ìžì—° ê²½ê´€"
        }
        
        # ìžì—°ì–´ ë¬¸ìž¥ ìƒì„±
        season_text = season_map.get(season, season)
        companion_text = companion_map.get(companion, companion)
        concept_text = concept_map.get(travel_concept, travel_concept)
        
        # í…œí”Œë¦¿ ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„±
        if all([destination, season_text, companion_text, concept_text]):
            query_text = f"{destination} {season_text}ì— {companion_text}ê³¼ ì—¬í–‰í•˜ê¸° ì¢‹ì€ {concept_text} ì¤‘ì‹¬ ê´€ë ¨ ê¸€"
        else:
            # ì¼ë¶€ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
            parts = []
            if destination:
                parts.append(destination)
            if season_text:
                parts.append(f"{season_text} ì—¬í–‰")
            if companion_text:
                parts.append(f"{companion_text} ì—¬í–‰")
            if concept_text:
                parts.append(concept_text)
            query_text = " ".join(parts)
        
        return query_text
    
    def retrieve_documents(self, query_json: Dict[str, Any], 
                          k: int = 5,
                          show_scores: bool = True) -> List[Dict[str, Any]]:
        """
        JSON ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query_json: ê²€ìƒ‰í•  ì¿¼ë¦¬ JSON
            k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            show_scores: ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ ì—¬ë¶€
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        # JSONì„ ìžì—°ì–´ ì¿¼ë¦¬ë¡œ ë³€í™˜
        query_text = self.build_query_text(query_json)
        print(f"Generated query: {query_text}")
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        results_with_scores = self.vectorstore.similarity_search_with_score(
            query_text, k=k
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for doc, score in results_with_scores:
            result = {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "title": doc.metadata.get("title", ""),
                "url": doc.metadata.get("url", ""),
                "chunk_id": doc.metadata.get("chunk_id", ""),
                "parent_doc_id": doc.metadata.get("parent_doc_id", ""),
                "section_index": doc.metadata.get("section_index", 0),
                "chunk_index": doc.metadata.get("chunk_index", 0),
                "similarity_score": score if show_scores else None
            }
            results.append(result)
        
        return results
    
    def retrieve_documents_multi_query(self, query_json: Dict[str, Any], 
                                       k: int = 5,
                                       show_scores: bool = True) -> List[Dict[str, Any]]:
        """
        ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ìœ¼ë¡œ ë” ì •í™•í•œ ê²°ê³¼ ë°˜í™˜
        
        Args:
            query_json: ê²€ìƒ‰í•  ì¿¼ë¦¬ JSON
            k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            show_scores: ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ ì—¬ë¶€
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        all_results = []
        seen_chunk_ids = set()
        
        destination = query_json.get("destination", "")
        season = query_json.get("season", "")
        companion = query_json.get("companion", "")
        travel_concept = query_json.get("TravelConcept", "")
        
        # í‚¤ì›Œë“œ ì¡°í•© ìƒì„±
        search_queries = []

        # ê³„ì ˆ + ë™ë°˜ìž ì¡°í•©
        if season and companion:
            season_companion_map = {
            # ðŸ ê°€ì„
            ("ê°€ì„", "ìš°ì •ì—¬í–‰"): [
                "ê°€ì„ ì¹œêµ¬ ì—¬í–‰", "ê°€ì„ ì¹œêµ¬ë¼ë¦¬", "9ì›” 10ì›” 11ì›” ì¹œêµ¬", "ê°€ì„ì²  ìš°ì •ì—¬í–‰",
                "ê°€ì„ ìš°ì • ì—¬í–‰ì§€", "ê°€ì„ ì¹œêµ¬ ì¶”ì²œ", "ê°€ì„ ë†€ëŸ¬ê°€ê¸° ì¢‹ì€ ê³³", "ë‹¨í’ ì¹œêµ¬ ì—¬í–‰", "ê°€ì„ ì¶”ì–µ ì—¬í–‰"
            ],
            ("ê°€ì„", "ì»¤í”Œì—¬í–‰"): [
                "ê°€ì„ ì»¤í”Œ ì—¬í–‰", "ê°€ì„ ì—°ì¸", "9ì›” 10ì›” ì»¤í”Œ", "ê°€ì„ ë°ì´íŠ¸ ì—¬í–‰", "ì—°ì¸ ë‹¨í’ ì—¬í–‰",
                "ê°€ì„ ë‘˜ì´ì„œ", "ë‹¨í’ë†€ì´ ì»¤í”Œ", "ë¡œë§¨í‹±í•œ ê°€ì„ ì—¬í–‰", "ì»¤í”Œ ê°€ì„ ê°ì„± ì—¬í–‰", "ê°€ì„ ê°ì„± ì½”ìŠ¤"
            ],
            ("ê°€ì„", "ê°€ì¡±ì—¬í–‰"): [
                "ê°€ì„ ê°€ì¡± ì—¬í–‰", "ê°€ì„ ì•„ì´", "9ì›” 10ì›” ê°€ì¡±", "ê°€ì¡±ê³¼ ë‹¨í’ êµ¬ê²½", "ê°€ì„ì²  ê°€ì¡±ì—¬í–‰ì§€",
                "ê°€ì„ ë¶€ëª¨ë‹˜ ì—¬í–‰", "ê°€ì„ ê°€ì¡± ë‚˜ë“¤ì´", "ë‹¨í’ ì‹œì¦Œ ê°€ì¡± ì—¬í–‰", "ê°€ì„ ì´ˆë“±í•™ìƒ ì—¬í–‰", "ê°€ì„ ìœ ì•„ ë™ë°˜ ì—¬í–‰"
            ],
            ("ê°€ì„", "í˜¼ìžì—¬í–‰"): [
                "ê°€ì„ í˜¼ìž ì—¬í–‰", "ê°€ì„ ì†”ë¡œ", "9ì›” 10ì›” í˜¼í–‰", "ê°€ì„ í˜¼ìž ë– ë‚˜ëŠ” ì—¬í–‰", "ê°€ì„ ë‚˜í™€ë¡œ ì—¬í–‰",
                "ë‹¨í’ í˜¼ìž ë³´ê¸°", "ê°€ì„ ê°ì„± í˜¼í–‰", "ê°€ì„ í˜¼ìž ížë§", "í˜¼ìž ê°€ëŠ” ê°€ì„ ëª…ì†Œ", "ê°€ì„ í˜¼ë°¥ ì—¬í–‰"
            ],

            # ðŸŒ¸ ë´„
            ("ë´„", "ìš°ì •ì—¬í–‰"): [
                "ë´„ ì¹œêµ¬ ì—¬í–‰", "ë²šê½ƒ ìš°ì • ì—¬í–‰", "3ì›” 4ì›” 5ì›” ì¹œêµ¬", "ë´„ì²  ìš°ì • ì—¬í–‰ì§€",
                "ë´„ ì†Œí’ ì¹œêµ¬ëž‘", "ë´„ê½ƒë†€ì´ ì¹œêµ¬", "ì¹œêµ¬ëž‘ ë²šê½ƒ", "ë”°ëœ»í•œ ë´„ ì¹œêµ¬ ì—¬í–‰", "ë´„ ìš°ì • ì‚¬ì§„ì—¬í–‰"
            ],
            ("ë´„", "ì»¤í”Œì—¬í–‰"): [
                "ë´„ ì»¤í”Œ ì—¬í–‰", "ë²šê½ƒ ë°ì´íŠ¸", "3ì›” 4ì›” 5ì›” ì—°ì¸", "ë´„ ì—°ì¸ ì¶”ì²œ ì—¬í–‰", "ë”°ëœ»í•œ ì»¤í”Œ ë´„ì—¬í–‰",
                "ë´„ ë°ì´íŠ¸ ì—¬í–‰ì§€", "ì»¤í”Œ ë²šê½ƒëª…ì†Œ", "ë´„ ê°ì„± ì»¤í”Œ", "ë´„ ë¡œë§¨í‹± ì—¬í–‰", "ë´„ì²  ì—°ì¸ ì—¬í–‰"
            ],
            ("ë´„", "ê°€ì¡±ì—¬í–‰"): [
                "ë´„ ê°€ì¡± ì—¬í–‰", "3ì›” 4ì›” 5ì›” ê°€ì¡±", "ë´„ì²  ê°€ì¡±", "ë´„ ë‚˜ë“¤ì´ ê°€ì¡±", "ë´„ê½ƒ ê°€ì¡± ì—¬í–‰",
                "ë²šê½ƒ ê°€ì¡±ì—¬í–‰", "ë´„ ê°€ì¡± ë‚˜ë“¤ì´ ì½”ìŠ¤", "ë´„ ìœ ì¹˜ì› ê°€ì¡±ì—¬í–‰", "ì–´ë¦°ì´ë‚  ì—¬í–‰", "ë´„ ì£¼ë§ ê°€ì¡± ë‚˜ë“¤ì´"
            ],
            ("ë´„", "í˜¼ìžì—¬í–‰"): [
                "ë´„ í˜¼ìž ì—¬í–‰", "ë´„ í˜¼ìž", "ë´„ ê°ì„± ë‚˜í™€ë¡œ", "ë²šê½ƒ í˜¼ìžë³´ê¸°", "í˜¼ìž ë– ë‚˜ëŠ” ë´„ ì—¬í–‰",
                "í˜¼ìž ë²šê½ƒê¸¸ ê±·ê¸°", "ë´„ ížë§ í˜¼í–‰", "ë´„ ê°ì„±", "ë´„ì²  ì†”ë¡œì—¬í–‰", "ë´„ í˜¼ë°¥ ì—¬í–‰"
            ],

            # â˜€ï¸ ì—¬ë¦„
            ("ì—¬ë¦„", "ìš°ì •ì—¬í–‰"): [
                "ì—¬ë¦„ ì¹œêµ¬ ì—¬í–‰", "6ì›” 7ì›” 8ì›” ì¹œêµ¬", "ì—¬ë¦„ì²  ìš°ì •ì—¬í–‰", "ì—¬ë¦„ ë°”ë‹¤ ì¹œêµ¬", "ì¹œêµ¬ë¼ë¦¬ ì›Œí„°íŒŒí¬",
                "ì—¬ë¦„ ê³„ê³¡ ì¹œêµ¬ì—¬í–‰", "ì—¬ë¦„ ìš°ì • ì—¬í–‰ì§€", "ì—¬ë¦„ ë¶ˆë© ì¹œêµ¬", "ì—¬ë¦„ MT ëŠë‚Œ ì—¬í–‰", "ì°œí†µ ë”ìœ„ ì¹œêµ¬ëž‘"
            ],
            ("ì—¬ë¦„", "ì»¤í”Œì—¬í–‰"): [
                "ì—¬ë¦„ ì»¤í”Œ ì—¬í–‰", "ì—¬ë¦„ ë°ì´íŠ¸", "ì—¬ë¦„ ì—°ì¸ ì¶”ì²œ", "7ì›” ì»¤í”Œ ë°”ìº‰ìŠ¤", "ì—¬ë¦„ ë°”ë‹¤ ì»¤í”Œ",
                "ì»¤í”Œ ê³„ê³¡ ì—¬í–‰", "ì—¬ë¦„ ê°ì„± ë°ì´íŠ¸", "ì—¬ë¦„ ì»¤í”Œ ë§›ì§‘", "ë¬´ë”ìœ„ í”¼ì„œ ì»¤í”Œ", "ì—¬ë¦„ì²  ì»¤í”Œ ì½”ìŠ¤"
            ],
            ("ì—¬ë¦„", "ê°€ì¡±ì—¬í–‰"): [
                "ì—¬ë¦„ ê°€ì¡± ì—¬í–‰", "ë°©í•™ ê°€ì¡± ì—¬í–‰", "ì—¬ë¦„ì²  ê°€ì¡±ì—¬í–‰ì§€", "ì•„ì´ë“¤ê³¼ ê³„ê³¡", "7ì›” 8ì›” ê°€ì¡± ì—¬í–‰",
                "ì—¬ë¦„ ë°”ë‹¤ ê°€ì¡±", "ë¬¼ë†€ì´ ê°€ì¡±ì—¬í–‰", "ê°€ì¡± í”¼ì„œì§€", "ì—¬ë¦„ ê°€ì¡± ë‚˜ë“¤ì´", "ì—¬ë¦„ ì—„ë§ˆ ì•„ë¹ ì™€ ì—¬í–‰"
            ],
            ("ì—¬ë¦„", "í˜¼ìžì—¬í–‰"): [
                "ì—¬ë¦„ í˜¼ìž ì—¬í–‰", "ì—¬ë¦„ í˜¼í–‰", "í˜¼ìž ì—¬ë¦„ ë°”ë‹¤", "í˜¼ìž ê³„ê³¡ ížë§", "ì—¬ë¦„ ë‚˜í™€ë¡œ ì—¬í–‰ì§€",
                "ì—¬ë¦„ í˜¼ìž ë¬¼ë†€ì´", "ì—¬ë¦„ í˜¼ìž ê°ì„±ì—¬í–‰", "ì—¬ë¦„ í˜¼ìž ë”ìœ„ í”¼í•˜ê¸°", "í˜¼ìž ì—¬í–‰ ì—¬ë¦„ ì¶”ì²œ", "ì—¬ë¦„ í˜¼ë°¥ ì—¬í–‰"
            ],

            # â„ï¸ ê²¨ìš¸
            ("ê²¨ìš¸", "ìš°ì •ì—¬í–‰"): [
                "ê²¨ìš¸ ì¹œêµ¬ ì—¬í–‰", "12ì›” 1ì›” 2ì›” ì¹œêµ¬", "ê²¨ìš¸ì²  ìš°ì • ì—¬í–‰", "ëˆˆ ì˜¤ëŠ” ë‚  ì¹œêµ¬ëž‘", "ìŠ¤í‚¤ìž¥ ì¹œêµ¬ë“¤",
                "ê²¨ìš¸ ì¹œêµ¬ ë¶ˆë©", "ê²¨ìš¸ ë°©í•™ ì¹œêµ¬ ì—¬í–‰", "ë”°ëœ»í•œ ìš°ì •ì—¬í–‰", "ê²¨ìš¸ ê°ì„± ì¹œêµ¬", "ê²¨ìš¸ MT ëŠë‚Œ"
            ],
            ("ê²¨ìš¸", "ì»¤í”Œì—¬í–‰"): [
                "ê²¨ìš¸ ì»¤í”Œ ì—¬í–‰", "12ì›” 1ì›” 2ì›” ì—°ì¸", "ê²¨ìš¸ì²  ì»¤í”Œ", "ê²¨ìš¸ ì»¤í”Œ ë°ì´íŠ¸", "ëˆˆ ì˜¤ëŠ” ì»¤í”Œ ì—¬í–‰",
                "ê²¨ìš¸ ëˆˆê½ƒ ì—¬í–‰", "ìŠ¤í‚¤ìž¥ ì»¤í”Œ", "ì—°ë§ ì»¤í”Œ ì—¬í–‰", "ê²¨ìš¸ ê°ì„± ë°ì´íŠ¸", "í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì»¤í”Œ ì—¬í–‰"
            ],
            ("ê²¨ìš¸", "ê°€ì¡±ì—¬í–‰"): [
                "ê²¨ìš¸ ê°€ì¡± ì—¬í–‰", "ê²¨ìš¸ ë°©í•™ ê°€ì¡±", "ê²¨ìš¸ì²  ê°€ì¡± ì—¬í–‰ì§€", "ëˆˆ ì˜¤ëŠ” ë‚  ê°€ì¡± ë‚˜ë“¤ì´", "ì˜¨ì²œ ê°€ì¡±ì—¬í–‰",
                "ê²¨ìš¸ ìŠ¤í‚¤ìž¥ ê°€ì¡±", "ê²¨ìš¸ ì•„ì´ë“¤ê³¼ ì—¬í–‰", "ê²¨ìš¸ ëˆˆì°ë§¤ ê°€ì¡±", "12ì›” 1ì›” ê°€ì¡± ì¶”ì²œ", "ê²¨ìš¸ ë”°ëœ»í•œ ê°€ì¡±ì—¬í–‰"
            ],
            ("ê²¨ìš¸", "í˜¼ìžì—¬í–‰"): [
                "ê²¨ìš¸ í˜¼ìž ì—¬í–‰", "ê²¨ìš¸ í˜¼í–‰", "ê²¨ìš¸ ë‚˜í™€ë¡œ ížë§", "ëˆˆ ë‚´ë¦¬ëŠ” ë‚  í˜¼ìž", "ê²¨ìš¸ ì˜¨ì²œ í˜¼ìž",
                "í˜¼ìž ë– ë‚˜ëŠ” ê²¨ìš¸", "ê²¨ìš¸ ê°ì„± í˜¼í–‰", "ì—°ë§ í˜¼ìž ì •ë¦¬ ì—¬í–‰", "ê²¨ìš¸ í˜¼ë°¥ ì½”ìŠ¤"
            ],
        }
            key = (season, companion)
            if key in season_companion_map:
                search_queries.extend(season_companion_map[key])
        
        # ê³„ì ˆ + ì»¨ì…‰ ì¡°í•©
        if season and travel_concept:
            season_concept_map = {
                ("ê°€ì„", "ì•¡í‹°ë¹„í‹°"): ["ê°€ì„ ì•¡í‹°ë¹„í‹°", "ê°€ì„ ì²´í—˜", "9ì›” 10ì›” í™œë™"],
                ("ê°€ì„", "ížë§"): ["ê°€ì„ ížë§", "ê°€ì„ íœ´ì‹", "9ì›” 10ì›” íœ´ì–‘"],
                ("ê°€ì„", "ë¬¸í™”"): ["ê°€ì„ ë¬¸í™”", "ê°€ì„ ë°•ë¬¼ê´€", "9ì›” 10ì›” ë¬¸í™”ê´€ê´‘"],
                ("ê°€ì„", "ìžì—°"): ["ê°€ì„ ìžì—°", "ê°€ì„ ê²½ê´€", "ë‹¨í’ ì–µìƒˆ"],
                ("ì—¬ë¦„", "ì•¡í‹°ë¹„í‹°"): ["ì—¬ë¦„ ì•¡í‹°ë¹„í‹°", "ì—¬ë¦„ í•´ìˆ˜ìš•", "6ì›” 7ì›” 8ì›” ì²´í—˜"],
                ("ë´„", "ìžì—°"): ["ë´„ ìžì—°", "ë´„ ë²šê½ƒ", "3ì›” 4ì›” 5ì›” ê²½ê´€"]
            }
            key = (season, travel_concept)
            if key in season_concept_map:
                search_queries.extend(season_concept_map[key])
        
        # ë™ë°˜ìž + ì»¨ì…‰ ì¡°í•©
        if companion and travel_concept:
            companion_concept_map = {
                ("ìš°ì •ì—¬í–‰", "ì•¡í‹°ë¹„í‹°"): ["ì¹œêµ¬ ì•¡í‹°ë¹„í‹°", "ì¹œêµ¬ ì²´í—˜", "ìš°ì •ì—¬í–‰ í™œë™"],
                ("ì»¤í”Œì—¬í–‰", "ížë§"): ["ì»¤í”Œ ížë§", "ì—°ì¸ íœ´ì–‘", "ë¡œë§¨í‹± íœ´ì‹"],
                ("ê°€ì¡±ì—¬í–‰", "ë¬¸í™”"): ["ê°€ì¡± ë¬¸í™”", "ê°€ì¡± ë°•ë¬¼ê´€", "ì•„ì´ ë¬¸í™”ê´€ê´‘"],
                ("í˜¼ìžì—¬í–‰", "ìžì—°"): ["í˜¼ìž ìžì—°", "ì†”ë¡œ ê²½ê´€", "í˜¼í–‰ í’ê²½"]
            }
            key = (companion, travel_concept)
            if key in companion_concept_map:
                search_queries.extend(companion_concept_map[key])
        
        # ì „ì²´ ì¡°í•© ì¿¼ë¦¬
        if season and companion and travel_concept:
            season_text = {"ê°€ì„": "ê°€ì„", "ì—¬ë¦„": "ì—¬ë¦„", "ë´„": "ë´„", "ê²¨ìš¸": "ê²¨ìš¸"}.get(season, season)
            companion_text = {"ìš°ì •ì—¬í–‰": "ì¹œêµ¬", "ì»¤í”Œì—¬í–‰": "ì»¤í”Œ", "ê°€ì¡±ì—¬í–‰": "ê°€ì¡±", "í˜¼ìžì—¬í–‰": "í˜¼ìž"}.get(companion, companion)
            concept_text = {"ì•¡í‹°ë¹„í‹°": "ì•¡í‹°ë¹„í‹°", "ížë§": "ížë§", "ë¬¸í™”": "ë¬¸í™”", "ìžì—°": "ìžì—°"}.get(travel_concept, travel_concept)
            
            search_queries.extend([
                f"{season_text} {companion_text} {concept_text}",
                f"{season_text} {companion_text} ì—¬í–‰ {concept_text}",
                f"{companion_text} {season_text} {concept_text} ì¶”ì²œ"
            ])
        
        # ê° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        for query in search_queries:
            try:
                full_query = f"{destination} {query}" if destination else query
                results_with_scores = self.vectorstore.similarity_search_with_score(full_query, k=k*2)
                
                for doc, score in results_with_scores:
                    chunk_id = doc.metadata.get("chunk_id", "")
                    if chunk_id and chunk_id not in seen_chunk_ids:
                        seen_chunk_ids.add(chunk_id)
                        
                        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                        content_lower = doc.page_content.lower()
                        title_lower = doc.metadata.get("title", "").lower()
                        
                        match_score = 0
                        query_words = query.lower().split()
                        for word in query_words:
                            if word in content_lower or word in title_lower:
                                match_score += 1
                        
                        match_ratio = match_score / len(query_words) if query_words else 0
                        
                        result = {
                            "content": doc.page_content,
                            "metadata": doc.metadata,
                            "title": doc.metadata.get("title", ""),
                            "url": doc.metadata.get("url", ""),
                            "chunk_id": chunk_id,
                            "parent_doc_id": doc.metadata.get("parent_doc_id", ""),
                            "section_index": doc.metadata.get("section_index", 0),
                            "chunk_index": doc.metadata.get("chunk_index", 0),
                            "matched_keyword": query,
                            "similarity_score": score if show_scores else None,
                            "keyword_match_ratio": match_ratio,
                            "keyword_match_count": match_score
                        }
                        all_results.append(result)
            except Exception as e:
                print(f"Search error for '{query}': {e}")
                continue
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ê³¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬
        all_results.sort(key=lambda x: (-x['keyword_match_ratio'], x['similarity_score']))
        
        return all_results[:k]
    
    def get_top_documents(self, query_json: Dict[str, Any], k: int = 5) -> List[Dict[str, Any]]:
        """
        ê¸°ë³¸ê²€ìƒ‰, ë‹¤ì¤‘ê²€ìƒ‰, í™•ìž¥ê²€ìƒ‰ì„ í†µí•©í•˜ì—¬ ìµœìƒìœ„ ë¬¸ì„œ ë°˜í™˜
        
        Args:
            query_json: ê²€ìƒ‰í•  ì¿¼ë¦¬ JSON
            k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            page_contentì™€ metadataë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        all_results = []
        seen_chunk_ids = set()
        
        print("Basic search...")
        basic_results = self.retrieve_documents(query_json, k=k*3, show_scores=True)
        for result in basic_results:
            chunk_id = result.get('chunk_id', '')
            if chunk_id and chunk_id not in seen_chunk_ids:
                seen_chunk_ids.add(chunk_id)
                all_results.append({
                    'result': result,
                    'search_type': 'basic',
                    'score': result.get('similarity_score', 999)
                })
        
        print("Multi-keyword search...")
        multi_results = self.retrieve_documents_multi_query(query_json, k=k*3, show_scores=True)
        for result in multi_results:
            chunk_id = result.get('chunk_id', '')
            if chunk_id and chunk_id not in seen_chunk_ids:
                seen_chunk_ids.add(chunk_id)
                all_results.append({
                    'result': result,
                    'search_type': 'multi',
                    'score': result.get('similarity_score', 999)
                })
        
        print("Extended search...")
        extended_results = self.retrieve_documents(query_json, k=k*4, show_scores=True)
        for result in extended_results:
            chunk_id = result.get('chunk_id', '')
            if chunk_id and chunk_id not in seen_chunk_ids:
                seen_chunk_ids.add(chunk_id)
                all_results.append({
                    'result': result,
                    'search_type': 'extended',
                    'score': result.get('similarity_score', 999)
                })
        
        # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬
        all_results.sort(key=lambda x: x['score'])
        
        # ìƒìœ„ kê°œ ì„ íƒí•˜ì—¬ í˜•ì‹ ë³€í™˜
        top_results = []
        for i, item in enumerate(all_results[:k]):
            result = item['result']
            
            formatted_result = {
                "page_content": result['content'],
                "metadata": {
                    "source": result.get('url', ''),
                    "title": result.get('title', ''),
                    "source_type": result['metadata'].get('source_type', 'unknown'),
                    "chunk_id": result.get('chunk_id', ''),
                    "similarity_score": result.get('similarity_score'),
                    "search_method": item['search_type'],
                    "rank": i + 1,
                    "original_metadata": result['metadata']
                }
            }
            top_results.append(formatted_result)
        
        print(f"Retrieved {len(top_results)} documents")
        return top_results

    def get_collection_stats(self) -> Dict[str, Any]:
        """
        ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´ ë°˜í™˜
        """
        try:
            count = self.vectorstore._collection.count()
            return {
                "total_documents": count,
                "collection_name": self.collection_name,
                "status": "active"
            }
        except Exception as e:
            return {
                "error": str(e),
                "collection_name": self.collection_name,
                "status": "error"
            }